{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# II.3 QR factorisation\n\nLet $A ∈ ℂ^{m × n}$ be a rectangular or square matrix such that $m ≥ n$ (i.e. more rows then columns).\nIn this chapter we consider two closely related factorisations:\n\n1. The _QR factorisation_\n$$\nA = Q R = \\underbrace{\\begin{bmatrix} 𝐪_1 | ⋯ | 𝐪_m \\end{bmatrix}}_{Q ∈ U(m)} \\underbrace{\\begin{bmatrix} × & ⋯ & × \\\\ & ⋱ & ⋮ \\\\ && × \\\\ &&0 \\\\ &&⋮ \\\\ && 0 \\end{bmatrix}}_{R ∈ ℂ^{m × n}}\n$$\nwhere $Q$ is unitary (i.e., $Q ∈ U(m)$, satisfying $Q^⋆Q = I$, with columns $𝐪_j ∈ ℂ^m$) and $R$ is _right triangular_, which means it \nis only nonzero on or to the right of the diagonal ($r_{kj} = 0$ if $k > j$).\n\n2. The _reduced QR factorisation_\n$$\nA = \\hat Q \\hat R = \\underbrace{\\begin{bmatrix} 𝐪_1 | ⋯ | 𝐪_n \\end{bmatrix}}_{ \\hat Q ∈ ℂ^{m × n}} \\underbrace{\\begin{bmatrix} × & ⋯ & × \\\\ & ⋱ & ⋮ \\\\ && ×  \\end{bmatrix}}_{\\hat R ∈ ℂ^{n × n}}\n$$\nwhere $Q$ has orthogonal columns ($Q^⋆ Q = I$, $𝐪_j ∈ ℂ^m$) and $\\hat R$ is upper triangular.\n\nNote for a square matrix the reduced QR factorisation is equivalent to the QR factorisation, in which case $R$ is _upper triangular_.\nThe importance of these decomposition for square matrices is that their component pieces are easy to invert:\n$$\nA = QR \\qquad ⇒ \\qquad A^{-1}𝐛 = R^{-1} Q^⊤ 𝐛\n$$\nand we saw in the last two chapters that triangular and orthogonal matrices are easy to invert when applied to a vector $𝐛$,\ne.g., using forward/back-substitution.\n\nFor rectangular matrices we will see that they lead to efficient solutions to the _least squares problem_: find\n$𝐱$ that minimizes the 2-norm\n$$\n\\| A 𝐱 - 𝐛 \\|.\n$$\nNote in the rectangular case the QR decomposition contains within it the reduced QR decomposition:\n$$\nA = QR = \\begin{bmatrix} \\hat Q | 𝐪_{n+1} | ⋯ | 𝐪_m \\end{bmatrix} \\begin{bmatrix} \\hat R \\\\  𝟎_{m-n × n} \\end{bmatrix} = \\hat Q \\hat R.\n$$\n\n\n\n\nIn this lecture we discuss the followng:\n\n1. QR and least squares: We discuss the QR decomposition and its usage in solving least squares problems.\n2. Reduced QR and Gram–Schmidt: We discuss computation of the Reduced QR decomposition using Gram–Schmidt.\n3. Householder reflections and QR: We discuss computing the  QR decomposition using Householder reflections."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using LinearAlgebra, Plots, BenchmarkTools"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. QR and least squares\n\nHere we consider rectangular matrices with more rows than columns. Given $A ∈ ℂ^{m × n}$ and $𝐛 ∈ ℂ^m$,\nleast squares consists of finding a vector $𝐱 ∈ ℂ^n$ that minimises the 2-norm:\n$\n\\| A 𝐱 - 𝐛 \\|\n$.\n\n\n**Theorem 1 (least squares via QR)** Suppose $A ∈ ℂ^{m × n}$ has full rank. Given a QR decomposition $A = Q R$\nthen\n$$\n𝐱 = \\hat R^{-1} \\hat Q^⋆ 𝐛\n$$\nminimises $\\| A 𝐱 - 𝐛 \\|$. \n\n**Proof**\n\nThe norm-preserving property (see PS4 Q3.1) of unitary matrices tells us\n$$\n\\| A 𝐱 - 𝐛 \\| = \\| Q R 𝐱 - 𝐛 \\| = \\| Q (R 𝐱 - Q^⋆ 𝐛) \\| = \\| R 𝐱 - Q^⋆ 𝐛 \\| = \\left \\| \n\\begin{bmatrix} \\hat R \\\\ 𝟎_{m-n × n} \\end{bmatrix} 𝐱 - \\begin{bmatrix} \\hat Q^⋆ \\\\ 𝐪_{n+1}^⋆ \\\\ ⋮ \\\\ 𝐪_m^⋆ \\end{bmatrix}     𝐛 \\right \\|\n$$\nNow note that the rows $k > n$ are independent of $𝐱$ and are a fixed contribution. Thus to minimise this norm it suffices to\ndrop them and minimise:\n$$\n\\| \\hat R 𝐱 - \\hat Q^⋆ 𝐛 \\|\n$$\nThis norm is minimised if it is zero. Provided the column rank of $A$ is full, $\\hat R$ will be invertible (Exercise: why is this?).\n\n∎\n\n\n**Example 1 (quadratic fit)** Suppose we want to fit noisy data by a quadratic\n$$\np(x) = p₀ + p₁ x + p₂ x^2\n$$\nThat is, we want to choose $p₀,p₁,p₂$ at data samples $x_1, …, x_m$ so that the following is true:\n$$\np₀ + p₁ x_k + p₂ x_k^2 ≈ f_k\n$$\nwhere $f_k$ are given by data. We can reinterpret this as a least squares problem: minimise the norm\n$$\n\\left\\| \\begin{bmatrix} 1 & x_1 & x_1^2 \\\\ ⋮ & ⋮ & ⋮ \\\\ 1 & x_m & x_m^2 \\end{bmatrix}\n\\begin{bmatrix} p₀ \\\\ p₁ \\\\ p₂ \\end{bmatrix} - \\begin{bmatrix} f_1 \\\\ ⋮ \\\\ f_m \\end{bmatrix} \\right \\|\n$$\nWe can solve this using the QR decomposition:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "m,n = 100,3\n\nx = range(0,1; length=m) # 100 points\nf = 2 .+ x .+ 2x.^2 .+ 0.1 .* randn.() # Noisy quadratic\n\nA = x .^ (0:2)'  # 100 x 3 matrix, equivalent to [ones(m) x x.^2]\nQ,\\hat R = qr(A)\n\\hat Q = Q[:,1:n] # Q represents full orthogonal matrix so we take first 3 columns\n\np₀,p₁,p₂ = \\hat R \\ \\hat Q'f"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can visualise the fit:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "p = x -> p₀ + p₁*x + p₂*x^2\n\nscatter(x, f; label=\"samples\", legend=:bottomright)\nplot!(x, p.(x); label=\"quadratic\")"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that `\\` with a rectangular system does least squares by default:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "A \\ f"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Reduced QR and Gram–Schmidt\n\n\nHow do we compute the QR decomposition? We begin with a method\nyou may have seen before in another guise. Write\n$$\nA = \\begin{bmatrix} 𝐚_1 | ⋯ | 𝐚_n \\end{bmatrix}\n$$\nwhere $𝐚_k ∈  ℂ^m$ and assume they are linearly independent ($A$ has full column rank).\n\n\n**Proposition 1 (Column spaces match)** Suppose $A = \\hat Q  \\hat R$ where $\\hat Q = [𝐪_1|…|𝐪_n]$\nhas orthogonal columns and $\\hat R$ is upper-triangular, and $A$ has full rank.\nThen the first $j$ columns of\n$\\hat Q$ span the same space as the first $j$ columns of $A$:\n$$\n\\hbox{span}(𝐚_1,…,𝐚_j) = \\hbox{span}(𝐪_1,…,𝐪_j).\n$$\n\n**Proof**\n\nBecause $A$ has full rank we know $\\hat R$ is invertible, i.e. its diagonal entries do not vanish: $r_{jj} ≠ 0$.\nIf $𝐯 ∈ \\hbox{span}(𝐚_1,…,𝐚_j)$ we have for $𝐜 ∈ ℂ^j$\n$$\n𝐯 = \\begin{bmatrix} 𝐚_1 | ⋯ | 𝐚_j \\end{bmatrix} 𝐜 = \n\\begin{bmatrix} 𝐪_1 | ⋯ | 𝐪_j \\end{bmatrix}  \\hat R[1:j,1:j] 𝐜 ∈ \\hbox{span}(𝐪_1,…,𝐪_j)\n$$\n while if $𝐰 ∈ \\hbox{span}(𝐪_1,…,𝐪_j)$ we have for $𝐝 ∈ ℝ^j$\n$$\n𝐰 = \\begin{bmatrix} 𝐪_1 | ⋯ | 𝐪_j \\end{bmatrix} 𝐝  =  \\begin{bmatrix} 𝐚_1 | ⋯ | 𝐚_j \\end{bmatrix} \\hat R[1:j,1:j]^{-1} 𝐝 ∈  \\hbox{span}(𝐚_1,…,𝐚_j).\n$$\n\n∎\n\n \nIt is possible to find $\\hat Q$ and $\\hat R$ the  using the _Gram–Schmidt algorithm_.\nWe construct it column-by-column:\n\n**Algorithm 1 (Gram–Schmidt)** For $j = 1, 2, …, n$ define\n$$\n\\begin{align*}\n𝐯_j &:= 𝐚_j - ∑_{k=1}^{j-1} \\underbrace{𝐪_k^⋆ 𝐚_j}_{r_{kj}} 𝐪_k \\\\\nr_{jj} &:= {\\|𝐯_j\\|} \\\\\n𝐪_j &:= {𝐯_j \\over r_{jj}}\n\\end{align*}\n$$\n\n**Theorem 2 (Gram–Schmidt and reduced QR)** Define $𝐪_j$ and $r_{kj}$ as in Algorithm 1\n(with $r_{kj} = 0$ if $k > j$). Then a reduced QR decomposition is given by:\n$$\nA = \\underbrace{\\begin{bmatrix} 𝐪_1 | ⋯ | 𝐪_n \\end{bmatrix}}_{ \\hat Q ∈ ℂ^{m × n}} \\underbrace{\\begin{bmatrix} r_{11} & ⋯ & r_{1n} \\\\ & ⋱ & ⋮ \\\\ && r_{nn}  \\end{bmatrix}}_{\\hat R ∈ ℂ^{n × n}}\n$$\n\n**Proof**\n\nWe first show that $\\hat Q$ has orthogonal columns. Assume that $𝐪_ℓ^⋆ 𝐪_k = δ_{ℓk}$ for $k,ℓ < j$. \nFor $ℓ < j$ we then have\n$$\n𝐪_ℓ^⋆ 𝐯_j = 𝐪_ℓ^⋆ 𝐚_j - ∑_{k=1}^{j-1}  𝐪_ℓ^⋆𝐪_k 𝐪_k^⋆ 𝐚_j = 0\n$$\nhence $𝐪_ℓ^⋆ 𝐪_j = 0$ and indeed $\\hat Q$ has orthogonal columns. Further: from the definition of $𝐯_j$ we find\n$$\n𝐚_j = 𝐯_j + ∑_{k=1}^{j-1} r_{kj} 𝐪_k = ∑_{k=1}^j r_{kj} 𝐪_k  = \\hat Q \\hat R 𝐞_j\n$$\n\n∎\n\n### Gram–Schmidt in action\n\nWe are going to compute the reduced QR of a random matrix"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "m,n = 5,4\nA = randn(m,n)\nQ,\\hat R = qr(A)\n\\hat Q = Q[:,1:n]"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first column of `\\hat Q` is indeed a normalised first column of `A`:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "R = zeros(n,n)\nQ = zeros(m,n)\nR[1,1] = norm(A[:,1])\nQ[:,1] = A[:,1]/R[1,1]"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now determine the next entries as"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "R[1,2] = Q[:,1]'A[:,2]\nv = A[:,2] - Q[:,1]*R[1,2]\nR[2,2] = norm(v)\nQ[:,2] = v/R[2,2]"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "And the third column is then:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "R[1,3] = Q[:,1]'A[:,3]\nR[2,3] = Q[:,2]'A[:,3]\nv = A[:,3] - Q[:,1:2]*R[1:2,3]\nR[3,3] = norm(v)\nQ[:,3] = v/R[3,3]"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Note the signs may not necessarily match.)\n\nWe can clean this up as a simple algorithm:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "function gramschmidt(A)\n    m,n = size(A)\n    m ≥ n || error(\"Not supported\")\n    R = zeros(n,n)\n    Q = zeros(m,n)\n    for j = 1:n\n        for k = 1:j-1\n            R[k,j] = Q[:,k]'*A[:,j]\n        end\n        v = A[:,j] - Q[:,1:j-1]*R[1:j-1,j]\n        R[j,j] = norm(v)\n        Q[:,j] = v/R[j,j]\n    end\n    Q,R\nend\n\nQ,R = gramschmidt(A)\nnorm(A - Q*R)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Complexity and stability\n\nWe see within the `for j = 1:n` loop that we have $O(mj)$ operations. Thus the \ntotal complexity is $O(m n^2)$ operations.\n\n\nUnfortunately, the Gram–Schmidt algorithm is _unstable_: the rounding errors when implemented in floating point\naccumulate in a way that we lose orthogonality:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "A = randn(300,300)\nQ,R = gramschmidt(A)\nnorm(Q'Q-I)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Householder reflections and QR\n\nAs an alternative, we will consider using Householder reflections to introduce zeros below\nthe diagonal.\nThus, if Gram–Schmidt is a process of _triangular orthogonalisation_ (using triangular matrices\nto orthogonalise), Householder reflections is a process of _orthogonal triangularisation_ \n(using orthogonal matrices to triangularise).\n\nConsider multiplication by the Householder reflection corresponding to the first column,\nthat is, for\n$$\nQ_1 := Q_{𝐚_1}^{\\rm H},\n$$\nconsider\n$$\nQ_1 A = \\begin{bmatrix} × & × & ⋯ & × \\\\\n& × & ⋯ & × \\\\\n                    & ⋮ & ⋱ & ⋮ \\\\\n                    & × & ⋯ & × \\end{bmatrix} = \n\\begin{bmatrix}  α & 𝐰^⊤ \\\\ \n& A_2   \\end{bmatrix}\n$$\nwhere \n$$\nα := -{\\rm csign}(a_{11})  \\|𝐚_1\\|, 𝐰 = (Q_1 A)[1, 2:n]  \\qquad \\hbox{and} \\qquad A_2 = (Q_1 A)[2:m, 2:n],\n$$\n${\\rm csign}(z) :=  {\\rm e}^{{\\rm i} \\arg z}$. \nThat is, we have made the first column triangular.\nIn terms of an algorithm, we then introduce zeros into the first column of $A_2$,\nleaving an $A_3$, and so-on. But we can wrap this iterative algorithm into a simple\nproof by induction:\n\n**Theorem 3 (QR)** \nEvery matrix $A ∈ ℂ^{m × n}$ has a QR factorisation:\n$$\nA = QR\n$$\nwhere $Q ∈ U(m)$ and $R ∈ ℂ^{m × n}$ is right triangular.\n\n**Proof**\n\nAssume $m ≥ n$. If $A = [𝐚_1] ∈ ℂ^{m × 1}$ then we have for the Householder\nreflection $Q_1 = Q_{𝐚_1}^{\\rm H}$\n$$\nQ_1 A = [α 𝐞₁]\n$$\nwhich is right triangular, where $α = -{\\rm sign}(a_{11}) \\|𝐚_1\\|$. \nIn other words \n$$\nA = \\underbrace{Q_1}_Q \\underbrace{[α 𝐞₁]}_R.\n$$\n\nFor $n > 1$, assume every matrix with less columns than $n$ has a QR factorisation.\nFor $A = [𝐚_1|…|𝐚_n] ∈ ℂ^{m × n}$, let $Q_1 = Q_{𝐚_1}^{\\rm H}$ so that\n$$\nQ_1 A =  \\begin{bmatrix} α & 𝐰^⊤ \\\\ & A_2 \\end{bmatrix}\n$$\nwhere $A_2 = (Q_1 A)[2:m,2:n]$ and $𝐰 = (Q_1 A)[1,2:n]$. By assumption $A_2 = Q̃ R̃$. Thus we have\n$$\n\\begin{align*}\nA = Q_1 \\begin{bmatrix} α & 𝐰^⊤ \\\\ & Q̃ R̃ \\end{bmatrix} \\\\\n=\\underbrace{Q_1 \\begin{bmatrix} 1 \\\\ & Q̃ \\end{bmatrix}}_Q  \\underbrace{\\begin{bmatrix} α & 𝐰^⊤ \\\\ &  R̃ \\end{bmatrix}}_R.\n\\end{align*}\n$$\n\n∎\n\nThis proof by induction leads naturally to an iterative algorithm. Note that $Q̃$ is a product of all\nHouseholder reflections that come afterwards, that is, we can think of $Q$ as:\n$$\nQ = Q_1 Q̃_2 Q̃_3 ⋯ Q̃_n\\qquad\\hbox{for}\\qquad Q̃_j = \\begin{bmatrix} I_{j-1} \\\\ & Q_j \\end{bmatrix}\n$$\nwhere $Q_j$ is a single Householder reflection corresponding to the first column of $A_j$. \nThis is stated cleanly in Julia code:\n\n**Algorithm 2 (QR via Householder)** For $A ∈ ℂ^{m × n}$ with $m ≥ n$, the QR factorisation can be implemented as follows:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "function householderreflection(x)\n    y = copy(x)\n    if x[1] == 0\n        y[1] += norm(x) \n    else # note sign(z) = exp(im*angle(z)) where `angle` is the argument of a complex number\n        y[1] += sign(x[1])*norm(x) \n    end\n    w = y/norm(y)\n    I - 2*w*w'\nend\nfunction householderqr(A)\n    T = eltype(A)\n    m,n = size(A)\n    if n > m\n        error(\"More columns than rows is not supported\")\n    end\n\n    R = zeros(T, m, n)\n    Q = Matrix(one(T)*I, m, m)\n    Aⱼ = copy(A)\n\n    for j = 1:n\n        𝐚₁ = Aⱼ[:,1] # first columns of Aⱼ\n        Q₁ = householderreflection(𝐚₁)\n        Q₁Aⱼ = Q₁*Aⱼ\n        α,𝐰 = Q₁Aⱼ[1,1],Q₁Aⱼ[1,2:end]\n        Aⱼ₊₁ = Q₁Aⱼ[2:end,2:end]\n\n        # populate returned data\n        R[j,j] = α\n        R[j,j+1:end] = 𝐰\n\n        # following is equivalent to Q = Q*[I 0 ; 0 Qⱼ]\n        Q[:,j:end] = Q[:,j:end]*Q₁\n\n        Aⱼ = Aⱼ₊₁ # this is the \"induction\"\n    end\n    Q,R\nend\n\nm,n = 100,50\nA = randn(m,n)\nQ,R = householderqr(A)\n@test Q'Q ≈ I\n@test Q*R ≈ A"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note because we are forming a full matrix representation of each Householder\nreflection this is a slow algorithm, taking $O(n^4)$ operations. The problem sheet\nwill consider a better implementation that takes $O(n^3)$ operations.\n\n\n**Example 2** We will now do an example by hand. Consider the $4 × 3$ matrix\n$$\nA = \\begin{bmatrix} \n4 & 2 & -1 \\\\ \n0 & 15 & 18 \\\\\n-2 & -4 & -4 \\\\\n-2 & -4 & -10\n\\end{bmatrix}\n$$\nFor the first column we have\n$$\nQ_1 = I - {1 \\over 12} \\begin{bmatrix} 4 \\\\ 0 \\\\ -2 \\\\ -2 \\end{bmatrix} \\begin{bmatrix} 4 & 0 & -2 & -2 \\end{bmatrix} =\n{1 \\over 3} \\begin{bmatrix}\n-1 & 0 & 2 & 2 \\\\\n0 & 3 & 0 & 0 \\\\\n2 & 0 & 2 & -1 \\\\\n2 & 0 & -1 &2\n\\end{bmatrix}\n$$\nso that\n$$\nQ_1 A = \\begin{bmatrix} -3 & -6 & -9 \\\\\n & 15 & 18 \\\\\n  & 0 & 0 \\\\\n& 0 & -6\n\\end{bmatrix}\n$$\nIn this example the next column is already upper-triangular,\nbut because of our choice of reflection we will end up swapping the sign, that is\n$$\nQ̃_2 = \\begin{bmatrix} 1 \\\\ & -1 \\\\ && 1 \\\\ &&& 1 \\end{bmatrix}\n$$\nso that\n$$\nQ̃_2 Q_1 A = \\begin{bmatrix} -3 & -6 & -9 \\\\\n & -15 & -18 \\\\\n  & 0 & 0 \\\\\n& 0 & -6\n\\end{bmatrix}\n$$\nThe final reflection is\n$$\nQ̃_3 = \\begin{bmatrix} I_{2 × 2} \\\\ &  I - \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix} \\begin{bmatrix} 1 & -1 \\end{bmatrix} \n\\end{bmatrix} = \\begin{bmatrix} å1 \\\\ & 1 \\\\ & & 0 & 1 \\\\ & & 1 & 0 \\end{bmatrix}\n$$\ngiving us\n$$\nQ̃_3 Q̃_2 Q_1 A = \\underbrace{\\begin{bmatrix} -3 & -6 & -9 \\\\\n & -15 & -18 \\\\\n  &  & -6 \\\\\n&  & 0\n\\end{bmatrix}}_R\n$$\nThat is,\n$$\nA = Q_1 Q̃_2 Q̃_3 R = \\underbrace{{1 \\over 3} \\begin{bmatrix}\n-1 & 0 & 2 & 2 \\\\\n0 & 3 & 0 & 0 \\\\\n2 & 0 & -1 & 2 \\\\\n2 & 0 & 2 &-1\n\\end{bmatrix}}_Q \\underbrace{\\begin{bmatrix} -3 & -6 & -9 \\\\\n & -15 & -18 \\\\\n  &  & -6 \\\\\n&  & 0\n\\end{bmatrix}}_R = \\underbrace{{1 \\over 3} \\begin{bmatrix}\n-1 & 0 & 2  \\\\\n0 & 3 & 0  \\\\\n2 & 0 & -1  \\\\\n2 & 0 & 2 \n\\end{bmatrix}}_\\hat Q  \\underbrace{\\begin{bmatrix} -3 & -6 & -9 \\\\\n & -15 & -18 \\\\\n  &  & -6 \n\\end{bmatrix}}_\\hat R\n$$"
      ],
      "metadata": {}
    }
  ],
  "nbformat_minor": 2,
  "metadata": {
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "1.8.3"
    },
    "kernelspec": {
      "name": "julia-1.8",
      "display_name": "Julia 1.8.3",
      "language": "julia"
    }
  },
  "nbformat": 4
}
